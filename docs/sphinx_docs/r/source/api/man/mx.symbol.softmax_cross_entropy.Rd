% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mxnet_generated.R
\name{mx.symbol.softmax_cross_entropy}
\alias{mx.symbol.softmax_cross_entropy}
\title{softmax_cross_entropy:Calculate cross entropy of softmax output and one-hot label.}
\usage{
mx.symbol.softmax_cross_entropy(...)
}
\arguments{
\item{data}{NDArray-or-Symbol
Input data}

\item{label}{NDArray-or-Symbol
Input label}

\item{name}{string, optional
Name of the resulting symbol.}
}
\value{
out The result mx.symbol
}
\description{
- This operator computes the cross entropy in two steps:
  - Applies softmax function on the input array.
  - Computes and returns the cross entropy loss between the softmax output and the labels.
}
\details{
- The softmax function and cross entropy loss is given by:

- Softmax Function:

.. math:: \text{softmax}(x)_i = \frac{exp(x_i)}{\sum_j exp(x_j)}

- Cross Entropy Function:

.. math:: \text{CE(label, output)} = - \sum_i \text{label}_i \log(\text{output}_i)

Example::

x = [[1, 2, 3],
       [11, 7, 5]]

label = [2, 0]

softmax(x) = [[0.09003057, 0.24472848, 0.66524094],
                [0.97962922, 0.01794253, 0.00242826]]

softmax_cross_entropy(data, label) = - log(0.66524084) - log(0.97962922) = 0.4281871



Defined in src/operator/loss_binary_op.cc:L59
}

