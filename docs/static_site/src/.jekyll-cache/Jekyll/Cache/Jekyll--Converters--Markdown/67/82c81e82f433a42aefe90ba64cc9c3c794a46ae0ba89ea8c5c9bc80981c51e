I"G<!--- Licensed to the Apache Software Foundation (ASF) under one -->
<!--- or more contributor license agreements.  See the NOTICE file -->
<!--- distributed with this work for additional information -->
<!--- regarding copyright ownership.  The ASF licenses this file -->
<!--- to you under the Apache License, Version 2.0 (the -->
<!--- "License"); you may not use this file except in compliance -->
<!--- with the License.  You may obtain a copy of the License at -->

<!---   http://www.apache.org/licenses/LICENSE-2.0 -->

<!--- Unless required by applicable law or agreed to in writing, -->
<!--- software distributed under the License is distributed on an -->
<!--- "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->
<!--- KIND, either express or implied.  See the License for the -->
<!--- specific language governing permissions and limitations -->
<!--- under the License. -->

<h1 id="optimizing-memory-consumption-in-deep-learning">Optimizing Memory Consumption in Deep Learning</h1>

<p>Over the last ten years, a constant trend in deep learning
is towards deeper and larger networks.
Despite rapid advances in hardware performance,
cutting-edge deep learning models continue to push the limits of GPU RAM.
So even today, it’s always desirable to find ways
to train larger models while consuming less memory.
Doing so enables us to train faster, using larger batch sizes,
and consequently achieving a higher GPU utilization rate.</p>

<p>In this document, we explore techniques for optimizing
memory allocation for deep neural networks.
We discuss a few candidate solutions.
While our proposals are by no means exhaustive,
these solutions are instructive and allow us to
introduce the major design issues at play.</p>

<h2 id="computation-graph">Computation Graph</h2>

<p>First, let’s revisit the idea of the computation graph.
A computation graph describes the (data flow) dependencies
between the operations in the deep network.
The operations performed in the graph
can be either fine-grained or coarse-grained.
The following figure shows two examples of computation graphs.</p>

<p><img src="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/memory/comp_graph_example.png" alt="Comp Graph Example" /></p>

<p>The concept of a computation graph is explicitly encoded in packages like Theano and CGT.
In other libraries, computation graphs appear implicitly as network configuration files.
The major difference in these libraries comes down to how they calculate gradients.
There are mainly two ways: performing back-propagation on the <em>same</em> graph
or explicitly representing a <em>backwards path</em> to calculate the required gradients.</p>

<p><img src="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/memory/back_graph.png" alt="Backward Graph" /></p>

<p>Libraries like Caffe, CXXNet, and Torch take the former approach,
performing back-prop on the original graph.
Libraries like Theano and CGT take the latter approach,
explicitly representing the backward path.
In this discussion, we adopt the <em>explicit backward path</em> approach
because it has several advantages for optimization.</p>

<p>However, we should emphasize that choosing the explicit backward path approach doesn’t restrict us
to symbolic libraries, such as Theano and CGT. We can also use the explicit backward path for gradient calculation of
layer-based (which ties forward and backward together) libraries. The following graph shows how to do this.
Basically, we introduce a backward node that links to the forward node of the graph and calls the <code class="highlighter-rouge">layer.backward</code>
in the backward operations.</p>

<p><img src="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/memory/explicit_back_layer.png" alt="Backward Layer" /></p>

<p>This discussion applies to almost all existing deep learning libraries.
(There are differences between libraries,  e.g., higher-order differentiation, which is beyond the scope of this topic.)</p>

<p>Why is the explicit backward path better? Let’s explain it with two examples.
The first reason is that the explicit backward path
clearly describes the dependency between computations.
Consider the following case, where we want to get
the gradient of A and B. As we can see clearly from the graph,
the computation of the <code class="highlighter-rouge">d(C)</code> gradient doesn’t depend on F.
This means that we can free the memory of <code class="highlighter-rouge">F</code>
right after the forward computation is done.
Similarly, the memory of <code class="highlighter-rouge">C</code> can be recycled.</p>

<p><img src="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/memory/back_dep_prune.png" alt="Backward Prune" /></p>

<p>Another advantage of the explicit backward path
is the ability to have a different backward path,
instead of a mirror of forward one.
A common example is the split connection case,
as shown in the following figure.</p>

<p><img src="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/memory/back_agg_grad.png" alt="Backward Agg" /></p>

<p>In this example, the output of B is referenced by two operations.
If we want to do the gradient calculation in the same
network, we need to introduce an explicit split layer.
This means we need to do the split for the forward pass, too.
In this figure, the forward pass doesn’t contain a split layer,
but the graph will automatically insert a gradient
aggregation node before passing the gradient back to B.
This helps us to save the memory cost of allocating the output of the split layer,
and the operation cost of replicating the data in the forward pass.</p>

<p>If we adopt the explicit backward approach,
there’s no difference between the forward pass and the backward pass.
We simply step through the computation graph in chronological order
and carry out computations.
This makes the explicit backward approach easy to analyze.
We just need to answer the question:
how do we allocate memory for each output node of a computation graph?</p>

<h2 id="what-can-be-optimized">What Can Be Optimized?</h2>

<p>As you can see, the computation graph is a useful way
to discuss memory allocation optimization techniques.
Already, we’ve shown how you can save some memory
by using the explicit backward graph.
Now let’s explore further optimizations,
and see how we might determine reasonable baselines for benchmarking.</p>

<p>Assume that we want to build a neural network with <code class="highlighter-rouge">n</code> layers.
Typically, when implementing a neural network,
we need to allocate node space for both the output of each layer
and the gradient values used during back-propagation.
This means we need roughly <code class="highlighter-rouge">2 n</code> memory cells.
We face the same requirement when using the explicit backward graph approach
because the number of nodes in a backward pass
is roughly the same as in a forward pass.</p>

<h3 id="in-place-operations">In-place Operations</h3>
<p>One of the simplest techniques we can employ
is <em>in-place memory sharing</em> across operations.
For neural networks, we can usually apply this technique
for the operations corresponding to activation functions.
Consider the following case, where we want
to compute the value of three chained sigmoid functions.</p>

<p><img src="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/memory/alloc_inline.png" alt="Inplace op" /></p>

<p>Because we can compute sigmoid <code class="highlighter-rouge">in-place</code>,
using the same memory for input and output,
we can compute an arbitrary-length chain
of sigmoid functions using constant memory.</p>

<p>Note: it’s easy to make mistakes when implementing in-place optimization.
Consider the following case, where the value of B is used not only by C, but also by F.</p>

<p><img src="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/memory/alloc_inline_trap.png" alt="In-place trap" /></p>

<p>We can’t perform in-place optimization because the value of B
is still needed after <code class="highlighter-rouge">C=sigmoid(B)</code> is computed.
An algorithm that simply does in-place optimization
for every sigmoid operation might fall into such trap,
so we need to be careful about when we can use it.</p>

<h3 id="standard-memory-sharing">Standard Memory Sharing</h3>
<p>In-place operations are not the only places where we can share memory.
In the following example, because the value of B is no longer needed
after we compute E, we can reuse B’s memory to hold the result of E.</p>

<p><img src="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/memory/alloc_normal.png" alt="Normal Sharing" /></p>

<p><em>Memory sharing doesn’t necessarily require the same data shape</em>.
Note that in the preceding example, the shapes of <code class="highlighter-rouge">B</code> and <code class="highlighter-rouge">E</code> can differ.
To handle such a situation, we can allocate a memory region
of size equal to the maximum of that required by <code class="highlighter-rouge">B</code> and <code class="highlighter-rouge">E</code> and share it between them.</p>

<h3 id="example-of-real-neural-network-allocation">Example of Real Neural Network Allocation</h3>
<p>Of course, these are only toy examples and they address only the computation of the forward pass.
But the same ideas apply to real neural networks.
The following figure shows an allocation plan for a two-layer perceptron.</p>

<p><img src="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/memory/alloc_mlp.png" alt="Net Alloc" /></p>

<p>In this example:</p>

<ul>
  <li>In-place optimization is applied when computing <code class="highlighter-rouge">act1</code>, <code class="highlighter-rouge">d(fc1)</code>, <code class="highlighter-rouge">out</code> and <code class="highlighter-rouge">d(fc2)</code>.</li>
  <li>Memory is shared between <code class="highlighter-rouge">d(act1)</code> and <code class="highlighter-rouge">d(A)</code>.</li>
</ul>

<h2 id="memory-allocation-algorithm">Memory Allocation Algorithm</h2>

<p>So far, we’ve discussed general techniques for optimizing memory allocation.
We’ve seen that there are traps to avoid,
as demonstrated in the case of in-place memory optimization.
So, how can we allocate memory correctly?
This is not a new problem.
For example, it is very similar
to the problem with register allocation in compilers.
There might be techniques that we can borrow.
We’re not attempting to give a comprehensive review of techniques here,
but rather to introduce some simple
but useful tricks to attack the problem.</p>

<p>The key problem is that we need to place resources
so that they don’t conflict with each other.
More specifically, each variable has a <em>life time</em>
between the time it gets computed until the last time it is used.
In the case of the multi-layer perceptron,
the <em>life time</em> of <code class="highlighter-rouge">fc1</code> ends after <code class="highlighter-rouge">act1</code> get computed.</p>

<p><img src="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/memory/alloc_mlp.png" alt="Net Alloc" /></p>

<p>The principle is <em>to allow memory sharing only between variables whose lifetimes don’t overlap</em>.
There are multiple ways to do this.
You can construct the conflicting graph
with each variable as a node and link the edge
between variables with overlapping lifespans,
and then run a graph-coloring algorithm.
This likely has <code class="highlighter-rouge">$O(n^2)$</code> complexity,
where <code class="highlighter-rouge">n</code> is the number of nodes in the graph.
This might be too costly.</p>

<p>Let’s consider another simple heuristic.
The idea is to simulate the procedure of traversing the graph,
and keep a count of future operations that depends on the node.</p>

<p><img src="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/memory/alloc_step.png" alt="Alloc" /></p>

<ul>
  <li>An in-place optimization can be performed when only the current operation depends on the source (i.e., <code class="highlighter-rouge">count==1</code>).</li>
  <li>Memory can be recycled into the box on the upper right corner when the <code class="highlighter-rouge">count</code> goes to 0.</li>
  <li>When we need new memory, we can either get it from the box or allocate a new one.</li>
</ul>

<p><strong><em>Note:</em></strong> During the simulation, no memory is allocated.
Instead, we keep a record of how much memory each node needs,
and allocate the maximum of the shared parts in the final memory plan.</p>

<h2 id="static-vs-dynamic-allocation">Static vs. Dynamic Allocation</h2>

<p>The preceding strategy exactly simulates
the dynamic memory allocation procedure
in imperative languages, such as Python.
The <code class="highlighter-rouge">count</code> is the reference counter for each memory object,
and the object gets garbage collected
when the reference counter goes to 0.
In that sense,
we are simulating dynamic memory allocation once
to create a static allocation plan.
Can we simply use an imperative language
that dynamically allocates and deallocates memory?</p>

<p>The major difference is that static allocation is only done once,
so we can afford to use more complicated algorithms.
For example, we can search for memory sizes
that are similar to the required memory block.
The Allocation can also be made graph aware.
We’ll talk about that in the next section.
Dynamic allocation puts more pressure
on fast memory allocation and garbage collection.</p>

<p>There is also one takeaway for users
who want to rely on dynamic memory allocations:
<em>do not unnecessarily reference objects</em>.
For example, if we organize all of the nodes in a list
and store then in a Net object,
these nodes will never get dereferenced, and we gain no space.
Unfortunately, this is a common way to organize code.</p>

<h2 id="memory-allocation-for-parallel-operations">Memory Allocation for Parallel Operations</h2>

<p>In the previous section, we discussed
how we can <em>simulate</em> running the procedure
for a computation graph to get a static allocation plan.
However, optimizing for parallel computation presents other challenges
because resource sharing and parallelization are on the two ends of a balance.
Let’s look at the following two allocation plans for the same graph:</p>

<p><img src="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/memory/parallel_alloc.png" alt="Parallel Alloc" /></p>

<p>Both allocation plans are valid
if we run the computation serially,
from <code class="highlighter-rouge">A[1]</code> to <code class="highlighter-rouge">A[8]</code>.
However, the allocation plan on the left
introduces additional dependencies,
which means we can’t run computation of <code class="highlighter-rouge">A[2]</code> and <code class="highlighter-rouge">A[5]</code> in parallel.
The plan on the right can.
To parallelize computation, we need to take greater care.</p>

<h3 id="be-correct-and-safe-first">Be Correct and Safe First</h3>
<p>Being correct is our first principle.
This means to execute in a way that takes implicit dependency
memory sharing into consideration.
You can do this by adding the implicit dependency edge to the execution graph.
Or, even simpler, if the execution engine is mutation aware,
as described in <a href="note_engine">our discussion of dependency engine design</a>,
push the operation in sequence
and write to the same variable tag
that represents the same memory region.</p>

<p>Always produce a safe memory allocation plan.
This means never allocate the same memory
to nodes that can be parallelized.
This might not be ideal when memory reduction is more desirable,
and we don’t gain too much when we can get benefit
from multiple computing streams simultaneously executing on the same GPU.</p>

<h3 id="try-to-allow-more-parallelization">Try to Allow More Parallelization</h3>
<p>Now we can safely perform some optimizations.
The general idea is to try and encourage memory sharing between nodes that can’t be parallelized.
You can do this by creating an ancestor relationship
graph and querying it during allocation,
which costs approximately <code class="highlighter-rouge">$O(n^2)$</code> in time to construct.
We can also use a heuristic here,
for example, color the path in the graph.
As shown in the following figure,
when you try to find the longest paths in the graph,
color them the same color and continue.</p>

<p><img src="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/memory/graph_color.png" alt="Path Color" /></p>

<p>After you get the color of the node,
you allow sharing (or encourage sharing)
only between nodes of the same color.
This is a stricter version of the ancestor relationship,
but it costs only <code class="highlighter-rouge">$O(n)$</code> of time
if you search for only the first <code class="highlighter-rouge">k</code> path.</p>

<p>This is by no means the only solution.
More sophisticated approaches might exist:</p>

<h2 id="how-much-can-you-save">How Much Can you Save?</h2>

<p>We’ve discussed the techniques and algorithms you can use
to squeeze memory usage for deep learning.
How much can you really save by using these techniques?</p>

<p>On coarse-grained operation graphs
that are already optimized for big operations,
you can reduce memory consumption roughly <em>by half</em>.
You can reduce memory usage even more
if you are optimizing a fine-grained computation network
used by symbolic libraries, such as Theano. Most of the ideas in this article inspired the design of <em>MXNet</em>.</p>

<p>Also, you will notice that memory cost, for forward pass only execution, is extremely low compared to running both forward and backward pass. This is simply because there’s  more memory reuse if you run only the forward pass.</p>

<p>So here are two takeaways:</p>

<ul>
  <li>Use a computation graph to allocate memory.</li>
  <li>For deep learning models, prediction consumes much less memory than training.</li>
</ul>

<h2 id="next-steps">Next Steps</h2>

<ul>
  <li><a href="note_data_loading">Efficient Data Loading Module for Deep Learning</a></li>
</ul>
:ET