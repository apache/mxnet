I"W<!--- Licensed to the Apache Software Foundation (ASF) under one -->
<!--- or more contributor license agreements.  See the NOTICE file -->
<!--- distributed with this work for additional information -->
<!--- regarding copyright ownership.  The ASF licenses this file -->
<!--- to you under the Apache License, Version 2.0 (the -->
<!--- "License"); you may not use this file except in compliance -->
<!--- with the License.  You may obtain a copy of the License at -->

<!---   http://www.apache.org/licenses/LICENSE-2.0 -->

<!--- Unless required by applicable law or agreed to in writing, -->
<!--- software distributed under the License is distributed on an -->
<!--- "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY -->
<!--- KIND, either express or implied.  See the License for the -->
<!--- specific language governing permissions and limitations -->
<!--- under the License. -->

<h1 id="why-was-mxnet-developed-in-the-first-place-">Why was MXNet developed in the first place ?</h1>

<p>Probably, if you’ve stumbled upon this page, you’ve heard of <em>deep learning</em>.
Deep learning denotes the modern incarnation of neural networks,
and it’s the technology behind recent breakthroughs
in self-driving cars, machine translation, speech recognition and more.
While widespread interest in deep learning took off in 2012,
deep learning has become an indispensable tool for countless industries.</p>

<p><img src="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/get-started/image-classification.png" alt="alt text" /></p>

<p>It might not come as a surprise that researchers
have investigated neural networks for decades.
Warren McCulloch and Walter Pitts
suggested the forerunner of today’s artificial neurons back in 1943.
Each neuron is connected to other neurons along <em>edges</em>, analogous to the synapses that connect real neurons.
And associated with each edge is a <em>weight</em> that indicates whether the connection is excitatory or inhibitatory and the strength of the connection.</p>

<p><img src="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/get-started/artificial-neuron-2.png" alt="alt_text" /></p>

<p>In the 1980s, the modern version of neural networks took shape.
Researchers arranged artificial neurons into <em>layers</em>.
Neurons in any layer get input from the neurons in the layers below them.
And, in turn, their output feeds into the neurons in the layer above.
Typically, the lowest layer represents the <em>input</em> to a neural network.
After computing the values of each layer, the <em>output</em> values are read out from the topmost layer.
The behavior of the network is determined by the setting of the weights.
And the process of <em>learning</em> in neural networks
is precisely the process of searching for good settings of these <em>weights</em>.</p>

<p>All that we need is an algorithm that tells us how to perform this search.
And since David Rumelhart and colleagues
introduced the <em>backpropagation</em> learning algorithm to train neural networks,
nearly all the major ideas have been in place.
Still, for many years neural networks took a backseat
to classical statistical methods like logistic regression and support vector machines (SVMs).
So you might reasonably ask, what’s changed to garner such interest?</p>

<h2 id="scale-and-computation">Scale and Computation</h2>
<p>The two biggest factors driving innovation in deep learning now are data and computation.
With distributed cloud computing and parallelism across GPU cores,
we can train models millions of times faster than researchers could in the 1980s.
The availability of large, high-quality datasets is another factor driving the field forward.
In the 1990s, the best datasets in computer vision had thousands of low-resolution images and ground truth assignments to a small number of classes.
Today, researchers cut their teeth on ImageNet, a massive dataset containing millions of high-resolution images from a thousand distinct classes.
The falling price of storage and high network bandwidth
make it affordable to work with big data at will.</p>

<p>In this new world, with bigger datasets and abundant computation,
neural networks dominate on most pattern recognition problems.
Over the last five years, neural networks have come to dominate on nearly every problem in computer vision,
replacing classical models and hand-engineered features.
Similarly, nearly every production speech recognition system now relies on neural networks,
where replacing the hidden Markov models that previously held sway.</p>

<p><img src="https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/get-started/nvidia-gpus.jpg" alt="alt text" /></p>

<p>While GPUs and clusters present a huge opportunity for accelerating neural network training,
adapting traditional machine learning code
to take advantage of these resources can be challenging.
The familiar scientific computing stacks (Matlab, R, or NumPy &amp; SciPy)
give no straight-forward way to exploit these distributed resources.</p>

<p>Acceleration libraries like <em>MXNet</em> offer powerful tools
to help developers exploit the full capabilities of GPUs and cloud computing.
While these tools are generally useful and applicable to any mathematical computation, <em>MXNet</em> places a special emphasis on speeding up the development and deployment of large-scale deep neural networks. In particular, we offer the following capabilities:</p>
<ul>
  <li><strong>Device Placement:</strong> With <em>MXNet</em>, it’s easy to specify where each data structures should live.</li>
  <li><strong>Multi-GPU training</strong>: <em>MXNet</em> makes it easy to scale computation with number of available GPUs.</li>
  <li><strong>Automatic differentiation</strong>: <em>MXNet</em> automates the derivative calculations that once bogged down neural network research.</li>
  <li><strong>Optimized Predefined Layers</strong>: While you can code up your own layers in <em>MXNet</em>, the predefined layers are optimized for speed, outperforming competing libraries.</li>
</ul>

<h2 id="deep-nets-on-fast-computers">Deep Nets on Fast Computers</h2>
<p>While MXNet can accelerate any numerical computation,
we developed the library with neural networks in mind.
However you plan to use MXNet, neural networks make for a powerful motivating example to display MXNet’s capabilities.</p>

<p>Neural networks are just functions for transforming input arrays <code class="highlighter-rouge">X</code> into output arrays <code class="highlighter-rouge">Y</code>.
In the case of image classification, <code class="highlighter-rouge">X</code> might represent the pixel values of an image, and <code class="highlighter-rouge">Y</code> might represent the corresponding probabilities that the image belongs to each of <code class="highlighter-rouge">10</code> classes.
For language translation, <code class="highlighter-rouge">X</code> and <code class="highlighter-rouge">Y</code> both might denote sequences of words. We’ll revisit the way you might represent sequences in subsequent tutorials - so for now it’s safe to think of <code class="highlighter-rouge">X</code> and <code class="highlighter-rouge">Y</code> as fixed length vectors.</p>

<p>To perform this mapping, neural networks stack <em>layers</em> of computation. Each layer consists of a linear function followed by a nonlinear transformation. In <em>MXNet</em> we might express this as:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">hidden_linear</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">sym</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>
<span class="n">hidden_activation</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">sym</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">hidden_linear</span><span class="p">)</span>
</code></pre></div></div>
<p>The linear transformations consist of multiplication by parameter arrays (<code class="highlighter-rouge">W</code> above).
When we talk about learning we mean finding the right set of values for <code class="highlighter-rouge">W</code>.
With just one layer, we can implement the familiar family of linear models,
including linear and logistic regression, linear support vector machines (SVMs), and the perceptron algorithm.
With more layers and a few clever constraints, we can implement all of today’s state-of-the-art deep learning techniques.</p>

<p>Of course, tens or hundreds of matrix multiplications can be computationally taxing.
Generally, these linear operations are the computational bottleneck.
Fortunately, linear operators can be parallelized trivially across the thousands of cores on a GPU.
But low-level GPU programming requires specialized skills that are not common even among leading researchers in the ML community. Moreover, even for CUDA experts, implementing a new neural network architecture shouldn’t require weeks of programming to implement low-level linear algebra operations. That’s where <em>MXNet</em> comes in.</p>
<ul>
  <li><em>MXNet</em> provides optimized numerical computation for GPUs and distributed ecosystems, from the comfort of high-level environments like Python and R</li>
  <li><em>MXNet</em> automates common workflows, so standard neural networks can be expressed concisely in just a few lines of code</li>
</ul>

<p>Now let’s take a closer look at the computational demands of neural networks
and give a sense of how <em>MXNet</em> helps us to write better, faster, code.
Say we have a neural network trained to recognize spam from the content of emails.
The emails may be streaming from an online service (at inference time),
or from a large offline dataset <strong>D</strong> (at training time).
In either case, the dataset typically must be managed by the CPU.</p>

<p><img src="https://raw.githubusercontent.com/kevinthesun/web-data/master/mxnet/get-started/architecture.png" alt="alt text" /></p>

<p>To compute the transformation of a neural network quickly, we need both the parameters and data points to make it into GPU memory. For any example <em>X</em>, the parameters <em>W</em> are the same. Moreover the size of the model tends to dwarf the size of an individual example. So we might arrive at the natural insight that parameters should always live on the GPU, even if the dataset itself must live on the CPU or stream in. This prevents IO from becoming the bottleneck during training or inference.</p>

<p>Fortunately, <em>MXNet</em> makes this kind of assignment easy.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">mxnet.ndarray</span> <span class="k">as</span> <span class="n">nd</span>

<span class="n">X</span>  <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">40000</span><span class="p">),</span> <span class="n">mx</span><span class="o">.</span><span class="n">cpu</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>           <span class="c1">#Allocate an array to store 1000 datapoints (of 40k dimensions) that lives on the CPU
</span><span class="n">W1</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">40000</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="n">mx</span><span class="o">.</span><span class="n">gpu</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>      <span class="c1">#Allocate a 40k x 1024 weight matrix on GPU for the 1st layer of the net
</span><span class="n">W2</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">mx</span><span class="o">.</span><span class="n">gpu</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>         <span class="c1">#Allocate a 1024 x 1024 weight matrix on GPU for the 2nd layer of the net
</span></code></pre></div></div>

<!-- * __Talk about how mxnet also makes it easy to assign a context (on which device the computation happens__ -->
<p>Similarly, <em>MXNet</em> makes it easy to specify the computing device</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">mx</span><span class="o">.</span><span class="n">Context</span><span class="p">(</span><span class="n">mx</span><span class="o">.</span><span class="n">gpu</span><span class="p">()):</span>          <span class="c1"># Absent this statement, by default, MXNet will execute on CPU
</span>    <span class="n">h</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">nd</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W1</span><span class="p">))</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nd</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">h1</span><span class="p">,</span> <span class="n">W2</span><span class="p">))</span>
</code></pre></div></div>

<p>Thus, with only a high-level understanding of how our numerical computation maps onto an execution environment, <em>MXNet</em> allows us to exert fine-grained control when needed.</p>

<h2 id="nuts-and-bolts">Nuts and Bolts</h2>

<p>MXNet supports two styles of programming: <em>imperative programming</em> (supported by the <em>NDArray</em> API) and <em>symbolic programming</em> (supported by the <em>Symbol</em> API). In short, imperative programming is the style that you’re likely to be most familiar with. Here if A and B are variables denoting matrices, then <code class="highlighter-rouge">C = A + B</code> is a piece of code that <em>when executed</em> sums the values referenced by <code class="highlighter-rouge">A</code> and <code class="highlighter-rouge">B</code> and stores their sum <code class="highlighter-rouge">C</code> in a new variable. Symbolic programming, on the other hand, allows functions to be defined abstractly through computation graphs. In the symbolic style, we first express complex functions in terms of placeholder values. Then, we can execute these functions by <em>binding them</em> to real values.</p>

<h3 id="imperative-programming-with-ndarray">Imperative Programming with <em>NDArray</em></h3>
<p>If you’re familiar with NumPy, then the mechanics of <em>NDArray</em> should be old hat. Like the corresponding <code class="highlighter-rouge">numpy.ndarray</code>, <code class="highlighter-rouge">mxnet.ndarray</code> (<code class="highlighter-rouge">mxnet.nd</code> for short) allows us to represent and manipulate multi-dimensional, homogenous arrays of fixed-size components. Converting between the two is effortless:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create a numpy array from an mxnet NDArray
</span><span class="n">A_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">],[</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">9</span><span class="p">]])</span>
<span class="n">A_nd</span> <span class="o">=</span> <span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>

<span class="c1"># Convert back to a numpy array
</span><span class="n">A2_np</span> <span class="o">=</span> <span class="n">A_nd</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
</code></pre></div></div>

<p>Other deep learning libraries tend to rely on NumPy exclusively for imperative programming and the syntax.
So you might reasonably wonder, why do we need to bother with <em>NDArray</em>?
Put simply, other libraries only reap the advantages of GPU computing when executing symbolic functions. By using <em>NDArray</em>, <em>MXNet</em> users can specify device context and run on GPUs. In other words, <em>MXNet</em> gives you access to the high-speed computation for imperative operations that Tensorflow and Theano only give for symbolic operations.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">]])</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">],[</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">]])</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">X</span> <span class="o">+</span> <span class="n">Y</span>
</code></pre></div></div>

<h3 id="symbolic-programming-in-mxnet">Symbolic Programming in <em>MXNet</em></h3>

<p>In addition to providing fast math operations through NDArray, <em>MXNet</em> provides an interface for defining operations abstractly via a computation graph.
With <code class="highlighter-rouge">mxnet.symbol</code>, we define operations abstractly in terms of place holders. For example, in the following code <code class="highlighter-rouge">a</code> and <code class="highlighter-rouge">b</code> stand in for real values that will be supplied at run time.
When we call <code class="highlighter-rouge">c = a+b</code>, no numerical computation is performed. This operation simply builds a graph that defines the relationship between <code class="highlighter-rouge">a</code>, <code class="highlighter-rouge">b</code> and <code class="highlighter-rouge">c</code>. In order to perform a real calculation, we need to bind <code class="highlighter-rouge">c</code> to real values.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">sym</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s">'a'</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">sym</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s">'b'</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
<span class="n">executor</span> <span class="o">=</span> <span class="n">c</span><span class="o">.</span><span class="n">bind</span><span class="p">(</span><span class="n">mx</span><span class="o">.</span><span class="n">cpu</span><span class="p">(),</span> <span class="p">{</span><span class="s">'a'</span><span class="p">:</span> <span class="n">X</span><span class="p">,</span> <span class="s">'b'</span><span class="p">:</span> <span class="n">Y</span><span class="p">})</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">executor</span><span class="o">.</span><span class="n">forward</span><span class="p">()</span>
</code></pre></div></div>

<p>Symbolic computation is useful for several reasons. First, because we define a full computation graph before executing it, <em>MXNet</em> can perform sophisticated optimizations to eliminate unnecessary or repeated work. This tends to give better performance than imperative programming. Second, because we store the relationships between different variables in the computation graph, <em>MXNet</em> can then perform efficient auto-differentiation.</p>

<p><strong>However</strong> Symbolic programming is error-prone and very slow to iterate with, as the graph needs to be computed before it is processed.</p>

<h3 id="gluon-for-briding-the-gap-between-the-two">Gluon for briding the gap between the two</h3>

<p><a href="/api/python">MXNet Gluon</a> aims to bridge the gap between the imperative nature of MXNet and its symbolic capabilities and keep the advantages of both through <a href="https://d2l.ai/chapter_computational-performance/hybridize.html">hybridization</a>.</p>

<h2 id="conclusions">Conclusions</h2>
<p>Given its combination of high performance, clean code, access to a high-level API, and low-level control, <em>MXNet</em> stands out as a unique choice among deep learning frameworks.</p>
:ET